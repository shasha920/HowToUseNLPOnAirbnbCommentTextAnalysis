{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAlgY7eCoD0f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('reviews.csv')\n",
        "\n",
        "# Extract the 'comments' column as the text corpus\n",
        "corpus = df['comments']\n",
        "\n",
        "# Ensure that the corpus is a list of strings\n",
        "corpus = df['comments'].astype(str).tolist()\n",
        "\n",
        "# Tokenize the text corpus\n",
        "corpus_tokens = [word_tokenize(comment) for comment in corpus]\n",
        "\n",
        "# Print the first few tokenized comments\n",
        "for tokens in corpus_tokens[:5]:\n",
        "    print(tokens)"
      ],
      "metadata": {
        "id": "VJ_v3Kjpq1ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words and perform cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "cleaned_corpus = []\n",
        "\n",
        "for tokens in corpus_tokens:\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Remove punctuation and convert to lowercase\n",
        "        token = token.lower()\n",
        "        if token.isalpha():\n",
        "            # Remove stop words\n",
        "            if token not in stop_words:\n",
        "                cleaned_tokens.append(token)\n",
        "    cleaned_corpus.append(cleaned_tokens)\n",
        "\n",
        "cleaned_corpus\n"
      ],
      "metadata": {
        "id": "L9oW7fQ8xVdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the corpus using lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "normalized_corpus = []\n",
        "\n",
        "for tokens in cleaned_corpus:\n",
        "    normalized_tokens = []\n",
        "    for token in tokens:\n",
        "        # Get the part of speech tag for each token\n",
        "        pos_tag = nltk.pos_tag([token])[0][1]\n",
        "        # Map the part of speech tag to WordNet tags\n",
        "        if pos_tag.startswith('J'):\n",
        "            wordnet_tag = wordnet.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            wordnet_tag = wordnet.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            wordnet_tag = wordnet.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            wordnet_tag = wordnet.ADV\n",
        "        else:\n",
        "            wordnet_tag = wordnet.NOUN\n",
        "        # Lemmatize the token\n",
        "        lemma = lemmatizer.lemmatize(token, pos=wordnet_tag)\n",
        "        normalized_tokens.append(lemma)\n",
        "    normalized_corpus.append(normalized_tokens)\n",
        "\n",
        "# Print the normalized corpus\n",
        "for tokens in normalized_corpus:\n",
        "    print(tokens)\n"
      ],
      "metadata": {
        "id": "IOVQ-CZdx5PW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}